# Mixture-of-Experts(MoE) Papers
![](https://img.shields.io/github/last-commit/Timothyxxx/RetrivalLMPapers?color=green)
Mixture of Experts Model papers.

## Papers

1. **GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.** 

   *Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen*  [[pdf](https://arxiv.org/abs/2006.16668)] 2020.6

2. **Beyond English-Centric Multilingual Machine Translation.** 

   *Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin*  [[pdf](https://arxiv.org/abs/2010.11125)] 2020.10

3. **Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.** 

   *William Fedus, Barret Zoph, Noam Shazeer*  [[pdf](https://arxiv.org/abs/2101.03961)] 2021.1

4. **BASE Layers: Simplifying Training of Large, Sparse Models.** 

   *Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, Luke Zettlemoyer*  [[pdf](https://arxiv.org/abs/2103.16716)] 2021.3
   
5. **DEMix Layers: Disentangling Domains for Modular Language Modeling.** 

   *Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, Luke Zettlemoyer*  [[pdf](https://arxiv.org/abs/2108.05036)] 2021.8

6. **Efficient Large Scale Language Modeling with Mixtures of Experts.** 

   *Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov*  [[pdf](https://arxiv.org/abs/2112.10684)] 2021.12

7. **GLaM: Efficient Scaling of Language Models with Mixture-of-Experts.** 

   *Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, Claire Cui*  [[pdf](https://arxiv.org/abs/2112.06905)] 2021.12
   
8. **DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale.** 

   *Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He*  [[pdf](https://arxiv.org/abs/2201.05596)] 2022.1
   
